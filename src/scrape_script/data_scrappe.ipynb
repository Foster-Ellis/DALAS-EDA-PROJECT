{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Causes More Scientific Discoveries in Short Time\n",
    "\n",
    "## Data Scrape\n",
    "\n",
    "- **Creating Author:** Balam, Yanheng Liu, Foster\n",
    "- **Latest Modification:** 20-03-2025  \n",
    "- **Modification Author:** Yanheng Liu  \n",
    "- **E-mail:** [yanheng.liu@etu.sorbonne-universite.fr](mailto:yanheng.liu@etu.sorbonne-universite.fr)  \n",
    "- **Version:** 1.1  \n",
    "\n",
    "---\n",
    "\n",
    "This is a data scrape provided for the project in *DALAS* course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check package whether are installed in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already installed: requests\n",
      "Already installed: beautifulsoup4\n",
      "Already installed: pandas\n",
      "Already installed: tabulate\n",
      "Already installed: pdfplumber\n",
      "Already installed: lxml\n",
      "Already installed: pandas\n",
      "Already installed: rapidfuzz\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "\n",
    "# Read package list from requirements.txt\n",
    "with open(\"../../requirements.txt\", \"r\") as file:\n",
    "    packages = [line.strip() for line in file if line.strip() and not line.startswith(\"#\")]\n",
    "\n",
    "# Get the list of currently installed packages\n",
    "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in packages:\n",
    "    pkg_name = package.split(\"==\")[0].lower() if \"==\" in package else package.lower()\n",
    "    if pkg_name not in installed_packages:\n",
    "        print(f\"Installing missing package: {package}\")\n",
    "        try:\n",
    "            subprocess.check_call([\"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Already installed: {package}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from lxml import html\n",
    "import pdfplumber\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Config & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, timeout=10):\n",
    "    \"\"\"\n",
    "    Fetch a webpage with basic error handling.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "            \" AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "            \" Chrome/113.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"ERROR: Failed to request page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Balam's Scrape Task\n",
    "\n",
    "Below is the web scraping process for Balam's part of the scraping in project.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w1():\n",
    "    url = \"https://unacademy.com/content/railway-exam/study-material/general-awareness/inventions-discoveries/\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        table = soup.find(\"table\")\n",
    "        \n",
    "        if table:\n",
    "            header_tags = table.find_all(\"th\")\n",
    "            if header_tags:\n",
    "                headers = [th.text.strip() for th in header_tags]\n",
    "            else:\n",
    "                headers = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "            \n",
    "            rows = []\n",
    "            for tr in table.find_all(\"tr\")[1:]: \n",
    "                cells = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "                if cells: \n",
    "                    rows.append(cells)\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Chart not found\")\n",
    "    else:\n",
    "        print(\"Error accesing to the website\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w2():\n",
    "    url = \"https://www.ipoi.gov.ie/en/student-teacher-zone/inventions-a-z/\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        content_wrapper = soup.find(\"div\", id=\"ContentWrapper\")\n",
    "        \n",
    "        if content_wrapper:\n",
    "            tables = content_wrapper.find_all(\"table\")  \n",
    "\n",
    "            all_data = []\n",
    "\n",
    "            for table in tables:\n",
    "                header_tags = table.find_all(\"th\")\n",
    "                if header_tags:\n",
    "                    headers = [th.text.strip() for th in header_tags]\n",
    "                else:\n",
    "                    headers = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "                \n",
    "                rows = []\n",
    "                for tr in table.find_all(\"tr\")[1:]: \n",
    "                    cells = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "                    if cells: \n",
    "                        rows.append(cells)\n",
    "\n",
    "                df1 = pd.DataFrame(rows, columns=headers)\n",
    "                all_data.append(df1)\n",
    "\n",
    "            if all_data:\n",
    "                df1 = pd.concat(all_data, ignore_index=True)\n",
    "                return df1\n",
    "            else:\n",
    "                print(\"No charts found\")\n",
    "        else:\n",
    "            print(\"Container not found\")\n",
    "    else:\n",
    "        print(\"Error accesing to the website\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w3():\n",
    "    url = \"https://www.adda247.com/defence-jobs/important-inventions-and-their-inventors/\"\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        tables = soup.find_all(\"table\")\n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        selected_tables = tables[1:3] if len(tables) > 2 else tables\n",
    "\n",
    "        for table in selected_tables:\n",
    "            rows = table.find_all(\"tr\")  \n",
    "            table_data = []\n",
    "\n",
    "            for row in rows:\n",
    "                cols = row.find_all([\"td\", \"th\"]) \n",
    "                cols = [col.text.strip() for col in cols]  \n",
    "                if cols:  \n",
    "                    table_data.append(cols)\n",
    "\n",
    "            if table_data:\n",
    "                all_data.extend(table_data)\n",
    "\n",
    "        df2 = pd.DataFrame(all_data)\n",
    "        df2.columns = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "        return df2\n",
    "\n",
    "    else:\n",
    "        print(\"Error acccesing to the website\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w4():\n",
    "    pdf_url = \"https://cdn1.byjus.com/wp-content/uploads/2020/06/List-of-Important-Inventions-Discoveries.pdf\"\n",
    "\n",
    "    pdf_response = requests.get(pdf_url)\n",
    "    pdf_path = \"../inventions_discoveries.pdf\"\n",
    "\n",
    "    with open(pdf_path, \"wb\") as file:\n",
    "        file.write(pdf_response.content)\n",
    "    table_data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                for row in table:\n",
    "                    if row and len(row) == 3 and \"Invention/Discovery\" not in row[0]:\n",
    "                        table_data.append(row)\n",
    "\n",
    "    all_data = []\n",
    "    if table_data:\n",
    "        all_data.extend(table_data)\n",
    "\n",
    "        df3 = pd.DataFrame(all_data)\n",
    "\n",
    "    df3.columns = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w5():\n",
    "    url = \"https://www.studyiq.com/articles/inventions-and-discoveries/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        entry_content = soup.find(\"div\", class_=\"entry-content\")\n",
    "\n",
    "        if not entry_content:\n",
    "            print(\"Entry-content not found\")\n",
    "        else:\n",
    "            tables = entry_content.find_all(\"table\")\n",
    "\n",
    "            if not tables or len(tables) < 3:\n",
    "                print(\"Error with one or more charts\")\n",
    "            else:\n",
    "                second_table = tables[1]  \n",
    "                third_table = tables[2]  \n",
    "\n",
    "                combined_data = []\n",
    "\n",
    "                for row in second_table.find_all(\"tr\"):\n",
    "                    cols = row.find_all([\"th\", \"td\"])\n",
    "                    cols = [col.text.strip() for col in cols]\n",
    "                    if cols and \"Invention\" not in cols:  \n",
    "                        combined_data.append(cols)\n",
    "\n",
    "                first_row = True  \n",
    "                for row in third_table.find_all(\"tr\"):\n",
    "                    cols = row.find_all([\"th\", \"td\"])\n",
    "                    cols = [col.text.strip() for col in cols]\n",
    "                    if cols:\n",
    "                        if first_row:\n",
    "                            first_row = False  \n",
    "                            continue\n",
    "                        combined_data.append(cols)\n",
    "\n",
    "                df4 = pd.DataFrame(combined_data, columns=[\"\",\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"])\n",
    "                return df4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file'../../raw_data/scrape_data/raw_data_balam_1.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_2.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_3.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_4.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_5.csv' saved\n"
     ]
    }
   ],
   "source": [
    "def general_file_creator():\n",
    "    dataframes = {\n",
    "        \"df\": scrapper_w1(),\n",
    "        \"df1\": scrapper_w2(),\n",
    "        \"df2\": scrapper_w3(),\n",
    "        \"df3\": scrapper_w4(),\n",
    "        \"df4\": scrapper_w5()\n",
    "    }\n",
    "\n",
    "    selected_columns = {\n",
    "        \"df\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],  \n",
    "        \"df1\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],\n",
    "        \"df2\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],\n",
    "        \"df3\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],\n",
    "        \"df4\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "    }\n",
    "\n",
    "    count = 1\n",
    "    for key, df in dataframes.items():\n",
    "        if key in selected_columns:\n",
    "            selected_cols = selected_columns[key]\n",
    "            \n",
    "            filtered_df = df[selected_cols] if all(col in df.columns for col in selected_cols) else df\n",
    "            \n",
    "            csv_filename = f\"../../raw_data/scrape_data/raw_data_balam_{count}.csv\"\n",
    "            count +=1\n",
    "            filtered_df.to_csv(csv_filename, index=False)\n",
    "            \n",
    "            print(f\"CSV file'{csv_filename}' saved\")\n",
    "\n",
    "general_file_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'combined_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, df1, df2, df3, df4], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df[combined_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m---> 11\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_dataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ CSV guardado como \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'combined_dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = scrapper_w1()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df1 = scrapper_w2()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df2 = scrapper_w3()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df3 = scrapper_w4()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df4 = scrapper_w5()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "\n",
    "combined_df = pd.concat([df, df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df[combined_df.iloc[:, 0] != combined_df.columns[0]]\n",
    "\n",
    "combined_df.to_csv(\"combined_dataset.csv\", index=False)\n",
    "print(\"Csv created'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Yanheng Liu's Scrape Task\n",
    "\n",
    "Below is the web scraping process for Yanheng Liu's part of the scraping in project.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
