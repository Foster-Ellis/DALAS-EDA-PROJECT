{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from lxml import html\n",
    "import pdfplumber\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w1():\n",
    "    url = \"https://unacademy.com/content/railway-exam/study-material/general-awareness/inventions-discoveries/\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        table = soup.find(\"table\")\n",
    "        \n",
    "        if table:\n",
    "            header_tags = table.find_all(\"th\")\n",
    "            if header_tags:\n",
    "                headers = [th.text.strip() for th in header_tags]\n",
    "            else:\n",
    "                headers = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "            \n",
    "            rows = []\n",
    "            for tr in table.find_all(\"tr\")[1:]: \n",
    "                cells = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "                if cells: \n",
    "                    rows.append(cells)\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Chart not found\")\n",
    "    else:\n",
    "        print(\"Error accesing to the website\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w2():\n",
    "    url = \"https://www.ipoi.gov.ie/en/student-teacher-zone/inventions-a-z/\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        content_wrapper = soup.find(\"div\", id=\"ContentWrapper\")\n",
    "        \n",
    "        if content_wrapper:\n",
    "            tables = content_wrapper.find_all(\"table\")  \n",
    "\n",
    "            all_data = []\n",
    "\n",
    "            for table in tables:\n",
    "                header_tags = table.find_all(\"th\")\n",
    "                if header_tags:\n",
    "                    headers = [th.text.strip() for th in header_tags]\n",
    "                else:\n",
    "                    headers = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "                \n",
    "                rows = []\n",
    "                for tr in table.find_all(\"tr\")[1:]: \n",
    "                    cells = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "                    if cells: \n",
    "                        rows.append(cells)\n",
    "\n",
    "                df1 = pd.DataFrame(rows, columns=headers)\n",
    "                all_data.append(df1)\n",
    "\n",
    "            if all_data:\n",
    "                df1 = pd.concat(all_data, ignore_index=True)\n",
    "                return df1\n",
    "            else:\n",
    "                print(\"No charts found\")\n",
    "        else:\n",
    "            print(\"Container not found\")\n",
    "    else:\n",
    "        print(\"Error accesing to the website\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w3():\n",
    "    url = \"https://www.adda247.com/defence-jobs/important-inventions-and-their-inventors/\"\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        tables = soup.find_all(\"table\")\n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        selected_tables = tables[1:3] if len(tables) > 2 else tables\n",
    "\n",
    "        for table in selected_tables:\n",
    "            rows = table.find_all(\"tr\")  \n",
    "            table_data = []\n",
    "\n",
    "            for row in rows:\n",
    "                cols = row.find_all([\"td\", \"th\"]) \n",
    "                cols = [col.text.strip() for col in cols]  \n",
    "                if cols:  \n",
    "                    table_data.append(cols)\n",
    "\n",
    "            if table_data:\n",
    "                all_data.extend(table_data)\n",
    "\n",
    "        df2 = pd.DataFrame(all_data)\n",
    "        df2.columns = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "        return df2\n",
    "\n",
    "    else:\n",
    "        print(\"Error acccesing to the website\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w4():\n",
    "    pdf_url = \"https://cdn1.byjus.com/wp-content/uploads/2020/06/List-of-Important-Inventions-Discoveries.pdf\"\n",
    "\n",
    "    pdf_response = requests.get(pdf_url)\n",
    "    pdf_path = \"../inventions_discoveries.pdf\"\n",
    "\n",
    "    with open(pdf_path, \"wb\") as file:\n",
    "        file.write(pdf_response.content)\n",
    "    table_data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                for row in table:\n",
    "                    if row and len(row) == 3 and \"Invention/Discovery\" not in row[0]:\n",
    "                        table_data.append(row)\n",
    "\n",
    "    all_data = []\n",
    "    if table_data:\n",
    "        all_data.extend(table_data)\n",
    "\n",
    "        df3 = pd.DataFrame(all_data)\n",
    "\n",
    "    df3.columns = [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_w5():\n",
    "    url = \"https://www.studyiq.com/articles/inventions-and-discoveries/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        entry_content = soup.find(\"div\", class_=\"entry-content\")\n",
    "\n",
    "        if not entry_content:\n",
    "            print(\"Entry-content not found\")\n",
    "        else:\n",
    "            tables = entry_content.find_all(\"table\")\n",
    "\n",
    "            if not tables or len(tables) < 3:\n",
    "                print(\"Error with one or more charts\")\n",
    "            else:\n",
    "                second_table = tables[1]  \n",
    "                third_table = tables[2]  \n",
    "\n",
    "                combined_data = []\n",
    "\n",
    "                for row in second_table.find_all(\"tr\"):\n",
    "                    cols = row.find_all([\"th\", \"td\"])\n",
    "                    cols = [col.text.strip() for col in cols]\n",
    "                    if cols and \"Invention\" not in cols:  \n",
    "                        combined_data.append(cols)\n",
    "\n",
    "                first_row = True  \n",
    "                for row in third_table.find_all(\"tr\"):\n",
    "                    cols = row.find_all([\"th\", \"td\"])\n",
    "                    cols = [col.text.strip() for col in cols]\n",
    "                    if cols:\n",
    "                        if first_row:\n",
    "                            first_row = False  \n",
    "                            continue\n",
    "                        combined_data.append(cols)\n",
    "\n",
    "                df4 = pd.DataFrame(combined_data, columns=[\"\",\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"])\n",
    "                return df4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file'../../raw_data/scrape_data/raw_data_balam_1.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_2.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_3.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_4.csv' saved\n",
      "CSV file'../../raw_data/scrape_data/raw_data_balam_5.csv' saved\n"
     ]
    }
   ],
   "source": [
    "def general_file_creator():\n",
    "    dataframes = {\n",
    "        \"df\": scrapper_w1(),\n",
    "        \"df1\": scrapper_w2(),\n",
    "        \"df2\": scrapper_w3(),\n",
    "        \"df3\": scrapper_w4(),\n",
    "        \"df4\": scrapper_w5()\n",
    "    }\n",
    "\n",
    "    selected_columns = {\n",
    "        \"df\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],  \n",
    "        \"df1\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],\n",
    "        \"df2\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],\n",
    "        \"df3\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"],\n",
    "        \"df4\": [\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]\n",
    "    }\n",
    "\n",
    "    count = 1\n",
    "    for key, df in dataframes.items():\n",
    "        if key in selected_columns:\n",
    "            selected_cols = selected_columns[key]\n",
    "            \n",
    "            filtered_df = df[selected_cols] if all(col in df.columns for col in selected_cols) else df\n",
    "            \n",
    "            csv_filename = f\"../../raw_data/scrape_data/raw_data_balam_{count}.csv\"\n",
    "            count +=1\n",
    "            filtered_df.to_csv(csv_filename, index=False)\n",
    "            \n",
    "            print(f\"CSV file'{csv_filename}' saved\")\n",
    "\n",
    "general_file_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv created'\n"
     ]
    }
   ],
   "source": [
    "df = scrapper_w1()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df1 = scrapper_w2()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df2 = scrapper_w3()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df3 = scrapper_w4()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "df4 = scrapper_w5()[[\"Invention/Discovery\", \"Name of the Inventor\", \"Year of Invention\"]]\n",
    "\n",
    "combined_df = pd.concat([df, df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df[combined_df.iloc[:, 0] != combined_df.columns[0]]\n",
    "\n",
    "combined_df.to_csv(\"../../raw_data/scrape_data/raw_data_balam.csv\", index=False)\n",
    "print(\"Csv created'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
