{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Causes More Scientific Discoveries in Short Time\n",
    "\n",
    "## Data Scrape\n",
    "\n",
    "- **Creating Author:** Yanheng Liu\n",
    "- **Latest Modification:** 20-03-2025  \n",
    "- **Modification Author:** Yanheng Liu  \n",
    "- **E-mail:** [yanheng.liu@etu.sorbonne-universite.fr](mailto:yanheng.liu@etu.sorbonne-universite.fr)  \n",
    "- **Version:** 1.1  \n",
    "\n",
    "---\n",
    "\n",
    "This is a data scrape provided for the project in *DALAS* course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check package whether are installed in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already installed: requests\n",
      "Already installed: beautifulsoup4\n",
      "Already installed: pandas\n",
      "Already installed: tabulate\n",
      "Already installed: pdfplumber\n",
      "Already installed: lxml\n",
      "Already installed: pandas\n",
      "Already installed: rapidfuzz\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "\n",
    "# Read package list from requirements.txt\n",
    "with open(\"../../requirements.txt\", \"r\") as file:\n",
    "    packages = [line.strip() for line in file if line.strip() and not line.startswith(\"#\")]\n",
    "\n",
    "# Get the list of currently installed packages\n",
    "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in packages:\n",
    "    pkg_name = package.split(\"==\")[0].lower() if \"==\" in package else package.lower()\n",
    "    if pkg_name not in installed_packages:\n",
    "        print(f\"Installing missing package: {package}\")\n",
    "        try:\n",
    "            subprocess.check_call([\"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Already installed: {package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from lxml import html\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Config & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, timeout=10):\n",
    "    \"\"\"\n",
    "    Fetch a webpage with basic error handling.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "            \" AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "            \" Chrome/113.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"ERROR: Failed to request page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Yanheng Liu's Scrape Task\n",
    "\n",
    "Below is the web scraping process for Yanheng Liu's part of the scraping in project.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. Crawler 1: Parse a specific table and save to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def parse_table_from_site1(html_content):\n",
    "    \"\"\"\n",
    "    Parse a specific <table> structure and return data as a list of [year, discovery_name, inventor].\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    table = soup.find(\"table\", {\n",
    "        \"width\": \"450\",\n",
    "        \"cellpadding\": \"5\",\n",
    "        \"cellspacing\": \"0\",\n",
    "        \"align\": \"left\"\n",
    "    })\n",
    "    if not table:\n",
    "        return []\n",
    "\n",
    "    rows = table.find_all(\"tr\", valign=\"top\")\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "        year = cols[0].get_text(strip=True)\n",
    "        second_cell = cols[1]\n",
    "        inventor_tag = second_cell.find(\"b\")\n",
    "        if inventor_tag:\n",
    "            inventor = inventor_tag.get_text(strip=True).replace(\"/\", \"\").strip()\n",
    "        else:\n",
    "            inventor = \"Unknown\"\n",
    "        name_text = second_cell.get_text(strip=True)\n",
    "        # Remove duplicate inventor string from discovery name\n",
    "        name = name_text.replace(inventor, \"\").replace(\"/\", \"\").strip()\n",
    "        data.append([year, name, inventor])\n",
    "    return data\n",
    "\n",
    "def write_to_csv_site1(data, filename):\n",
    "    \"\"\"\n",
    "    Write Site1 data to CSV.\n",
    "    Columns: [\"Year\", \"Name of Inventor\", \"Name of Invention\"]\n",
    "    \"\"\"\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Year\", \"Name of Inventor\", \"Name of Invention\"])\n",
    "        writer.writerows(data)\n",
    "\n",
    "def crawl_site1():\n",
    "    \"\"\"\n",
    "    Fetch page, parse table, and save CSV for Site1.\n",
    "    \"\"\"\n",
    "    url = \"https://press.uchicago.edu/Misc/Chicago/284158.html\"\n",
    "    html_content = fetch_page(url)\n",
    "    if html_content:\n",
    "        table_data = parse_table_from_site1(html_content)\n",
    "        write_to_csv_site1(table_data, \"../../raw_data/scrape_data/raw_data_yann_1.csv\")\n",
    "        print(\"Site1 data extraction completed, saved to raw_data_1.csv\")\n",
    "    else:\n",
    "        print(\"ERROR: Failed to retrieve HTML content for Site1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl for Site1...\n",
      "Site1 data extraction completed, saved to raw_data_1.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting crawl for Site1...\")\n",
    "crawl_site1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timeline_blocks_from_site2(page_html):\n",
    "    \"\"\"\n",
    "    Find the container with id=\"wphtsp-history-design-1\" and parse timeline blocks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = html.fromstring(page_html)\n",
    "        container_xpath = '//div[@id=\"wphtsp-history-design-1\"]'\n",
    "        container = tree.xpath(container_xpath)\n",
    "        if not container:\n",
    "            print(\"ERROR: Timeline container not found for Site2. Check the XPath!\")\n",
    "            return []\n",
    "        blocks = container[0].xpath('.//div[contains(@class, \"wphtsp-timeline-block\")]')\n",
    "        if not blocks:\n",
    "            print(\"WARNING: No timeline blocks found for Site2!\")\n",
    "        return blocks\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to parse HTML for Site2: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_block_site2(block):\n",
    "    \"\"\"\n",
    "    Parse a timeline block: title in the format \"year: name\" and content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title_elem = block.xpath('.//h2[@class=\"wphtsp-content-title\"]/a')\n",
    "        if not title_elem:\n",
    "            print(\"WARNING: Title <a> element not found in a block, skipping for Site2.\")\n",
    "            return None\n",
    "\n",
    "        title_text = title_elem[0].text_content().strip()\n",
    "        if ':' not in title_text:\n",
    "            print(f\"WARNING: Title format unexpected: {title_text}\")\n",
    "            return None\n",
    "\n",
    "        year_part, name_part = [part.strip() for part in title_text.split(\":\", 1)]\n",
    "\n",
    "        content_elem = block.xpath('.//div[contains(@class, \"wphtsp-content-inner\")]')\n",
    "        content_text = content_elem[0].text_content().strip() if content_elem else \"\"\n",
    "\n",
    "        return {\n",
    "            'Year': year_part,\n",
    "            'Name of Invention': name_part,\n",
    "            'Content': content_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error parsing a timeline block for Site2: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_to_csv_site2(data, filename='raw_data_2.csv'):\n",
    "    \"\"\"\n",
    "    Write Site2 results to CSV with fields ['Year', 'Name of Invention', 'Content'].\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"WARNING: No data to write to CSV for Site2.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['Year', 'Name of Invention', 'Content']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for item in data:\n",
    "                writer.writerow(item)\n",
    "        print(f\"Data successfully written to {filename} for Site2.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to write CSV for Site2: {e}\")\n",
    "\n",
    "def crawl_site2():\n",
    "    \"\"\"\n",
    "    Fetch page, parse timeline blocks, and write CSV for Site2.\n",
    "    \"\"\"\n",
    "    url = \"https://www.missingtheforest.com/history-of-science-timeline/\"\n",
    "    page_html = fetch_page(url)\n",
    "    if not page_html:\n",
    "        print(\"ERROR: Failed to retrieve HTML content for Site2.\")\n",
    "        return\n",
    "\n",
    "    blocks = parse_timeline_blocks_from_site2(page_html)\n",
    "    if not blocks:\n",
    "        print(\"ERROR: No timeline blocks found for Site2. Crawler will stop.\")\n",
    "        return\n",
    "\n",
    "    parsed_data = list(filter(None, map(parse_block_site2, blocks)))\n",
    "    write_to_csv_site2(parsed_data, \"../../raw_data/scrape_data/raw_data_yann_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl for Site2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ../../raw_data/scrape_data/raw_data_yann_2.csv for Site2.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting crawl for Site2...\")\n",
    "crawl_site2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Crawler 3: Parse <li> elements and save to CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def parse_li_elements_from_site3(page_html):\n",
    "    \"\"\"\n",
    "    Parse specific <li> elements using XPath.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = html.fromstring(page_html)\n",
    "        li_xpath = '//*[@id=\"mw-content-text\"]/div[1]/ul/li'\n",
    "        li_elements = tree.xpath(li_xpath)\n",
    "        if not li_elements:\n",
    "            print(\"WARNING: No <li> elements found on Site3. Check the XPath or page structure.\")\n",
    "        return li_elements\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to parse HTML for Site3: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_item_site3(li_element):\n",
    "    \"\"\"\n",
    "    Extract the year from the <b> tag and the full text from an <li> element.\n",
    "    Remove the duplicate year text from the content and omit any appended link information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract year from the <b> tag\n",
    "        year_elem = li_element.xpath('.//b/text()')\n",
    "        if not year_elem:\n",
    "            return None\n",
    "        year_text = year_elem[0].strip()\n",
    "        \n",
    "        # Get the full text content of the <li> element\n",
    "        raw_text = li_element.xpath('string(.)').strip()\n",
    "        \n",
    "        # Remove the year prefix from the raw_text if present\n",
    "        if raw_text.startswith(year_text):\n",
    "            raw_text = raw_text[len(year_text):].strip()\n",
    "        # Remove any leading punctuation such as a colon (\":\")\n",
    "        if raw_text.startswith(\":\"):\n",
    "            raw_text = raw_text[1:].strip()\n",
    "        \n",
    "        return {\n",
    "            'Year': year_text,\n",
    "            'Content': raw_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error parsing an item for Site3: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_to_csv_site3(data, filename='../../raw_data/scrape_data/raw_data_yann_3.csv'):\n",
    "    \"\"\"\n",
    "    Write Site3 results to CSV with fields ['Year', 'Content'].\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"WARNING: No data to write to CSV for Site3.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['Year', 'Content']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for item in data:\n",
    "                writer.writerow(item)\n",
    "        print(f\"Data successfully written to {filename} for Site3.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to write CSV for Site3: {e}\")\n",
    "\n",
    "def crawl_site3():\n",
    "    \"\"\"\n",
    "    Fetch the page, parse <li> elements, and write the CSV for Site3.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/Timeline_of_historic_inventions\"\n",
    "    page_html = fetch_page(url)\n",
    "    if not page_html:\n",
    "        print(\"ERROR: Failed to retrieve HTML content for Site3.\")\n",
    "        return\n",
    "\n",
    "    li_elements = parse_li_elements_from_site3(page_html)\n",
    "    if not li_elements:\n",
    "        print(\"ERROR: No <li> elements found. Aborting crawl for Site3.\")\n",
    "        return\n",
    "\n",
    "    parsed_data = list(filter(None, map(parse_item_site3, li_elements)))\n",
    "    write_to_csv_site3(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl for Site3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ../../raw_data/scrape_data/raw_data_yann_3.csv for Site3.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting crawl for Site3...\")\n",
    "crawl_site3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
