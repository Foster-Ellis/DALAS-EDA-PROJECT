{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60585118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2574380\n",
      "Splitting into 3 files of ~858127 rows each\n",
      "Chunk 1: 733014 rows → ../../raw_data/scrape_data\\patents_foster_1900-1909.csv\n",
      "Chunk 2: 824009 rows → ../../raw_data/scrape_data\\patents_foster_1910-1919.csv\n",
      "Chunk 3: 1017357 rows → ../../raw_data/scrape_data\\patents_foster_1920-1928.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "MAX_ROWS_PER_FILE = 1_000_000\n",
    "OUTPUT_DIR = \"../../raw_data/scrape_data\"\n",
    "FILE_PREFIX = \"patents_foster_\"\n",
    "CENTURY_PREFIX = \"19\"\n",
    "\n",
    "# Get all 19th-century files\n",
    "csv_files_19th_century = glob.glob(os.path.join(OUTPUT_DIR, f\"{FILE_PREFIX}{CENTURY_PREFIX}*.csv\"))\n",
    "\n",
    "# Define expected dtypes\n",
    "dtypes = {\n",
    "    \"year\": int,\n",
    "    \"country\": str,\n",
    "    \"doc_number\": str\n",
    "}\n",
    "\n",
    "# Load data\n",
    "dfs = [pd.read_csv(file, dtype=dtypes) for file in csv_files_19th_century]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "total_rows = len(combined_df)\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "\n",
    "if total_rows <= MAX_ROWS_PER_FILE:\n",
    "    # Save as single file\n",
    "    out_name = os.path.join(OUTPUT_DIR, f\"{FILE_PREFIX}1800-1899.csv\")\n",
    "    combined_df.to_csv(out_name, index=False)\n",
    "    print(f\"Combined file created: {out_name}\")\n",
    "else:\n",
    "    # Split into equal chunks\n",
    "    num_parts = math.ceil(total_rows / MAX_ROWS_PER_FILE)\n",
    "    chunk_size = math.ceil(total_rows / num_parts)\n",
    "    print(f\"Splitting into {num_parts} files of ~{chunk_size} rows each\")\n",
    "\n",
    "    years = sorted(combined_df[\"year\"].unique())\n",
    "    min_year = min(years)\n",
    "    max_year = max(years)\n",
    "    years_per_chunk = math.ceil((max_year - min_year + 1) / num_parts)\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        start_year = min_year + i * years_per_chunk\n",
    "        end_year = min(start_year + years_per_chunk - 1, max_year)\n",
    "\n",
    "        chunk_df = combined_df[(combined_df[\"year\"] >= start_year) & (combined_df[\"year\"] <= end_year)]\n",
    "\n",
    "        out_name = os.path.join(OUTPUT_DIR, f\"{FILE_PREFIX}{start_year}-{end_year}.csv\")\n",
    "        chunk_df.to_csv(out_name, index=False)\n",
    "        print(f\"Chunk {i+1}: {len(chunk_df)} rows → {out_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
