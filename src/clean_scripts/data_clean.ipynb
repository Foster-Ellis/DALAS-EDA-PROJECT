{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Causes More Scientific Discoveries in Short Time\n",
    "\n",
    "## Data Clean\n",
    "\n",
    "- **Creating Author:** Balam, Yanheng Liu, Foster\n",
    "- **Latest Modification:** 20-03-2025  \n",
    "- **Modification Author:** Yanheng Liu  \n",
    "- **E-mail:** [yanheng.liu@etu.sorbonne-universite.fr](mailto:yanheng.liu@etu.sorbonne-universite.fr)  \n",
    "- **Version:** 1.1  \n",
    "\n",
    "---\n",
    "\n",
    "This is a data clean provided for the project in *DALAS* course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check package whether are installed in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already installed: requests\n",
      "Already installed: beautifulsoup4\n",
      "Already installed: pandas\n",
      "Already installed: tabulate\n",
      "Already installed: pdfplumber\n",
      "Already installed: lxml\n",
      "Already installed: pandas\n",
      "Already installed: rapidfuzz\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "\n",
    "# Read package list from requirements.txt\n",
    "with open(\"../../requirements.txt\", \"r\") as file:\n",
    "    packages = [line.strip() for line in file if line.strip() and not line.startswith(\"#\")]\n",
    "\n",
    "# Get the list of currently installed packages\n",
    "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in packages:\n",
    "    pkg_name = package.split(\"==\")[0].lower() if \"==\" in package else package.lower()\n",
    "    if pkg_name not in installed_packages:\n",
    "        print(f\"Installing missing package: {package}\")\n",
    "        try:\n",
    "            subprocess.check_call([\"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Already installed: {package}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Function: preprocess_text\n",
    "# ----------------------------\n",
    "def preprocess_text(text: str, stopwords: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess input text by:\n",
    "    - Converting to lower case.\n",
    "    - Removing punctuation.\n",
    "    - Removing extra whitespace.\n",
    "    - Removing specified stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text string.\n",
    "        stopwords (list): List of stopwords to remove from text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation using translation table\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords if provided\n",
    "    if stopwords:\n",
    "        # Build regex pattern to match whole words in the stopwords list\n",
    "        pattern = r'\\b(?:' + '|'.join(map(re.escape, stopwords)) + r')\\b'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        # Remove extra spaces created by stopword removal\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# ----------------------------\n",
    "# Function: deduplicate_discoveries\n",
    "# ----------------------------\n",
    "def deduplicate_discoveries(df: pd.DataFrame, threshold: int = 90) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicate records based on the 'Name of Scientific Discovery' field using fuzzy matching.\n",
    "    \n",
    "    The function preprocesses the discovery names, compares them pairwise using a similarity threshold,\n",
    "    assigns group IDs for duplicates, and aggregates the groups by combining information from other fields.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing the data.\n",
    "        threshold (int): Similarity threshold (0-100) above which two strings are considered duplicates.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A deduplicated DataFrame with aggregated fields.\n",
    "    \"\"\"\n",
    "    # Define a custom stopwords list - can be modified or extended as needed\n",
    "    stopwords = ['the', 'for', 'of', 'origin', 'universe', 'theory', 'proposed']\n",
    "    \n",
    "    # Create a new column with cleaned 'Name of Scientific Discovery'\n",
    "    df['cleaned_discovery'] = df['Name of Scientific Discovery'].apply(lambda x: preprocess_text(x, stopwords=stopwords))\n",
    "    \n",
    "    # Initialize group IDs for each record as -1 (unassigned)\n",
    "    n = len(df)\n",
    "    group_ids = [-1] * n\n",
    "    current_group = 0\n",
    "    \n",
    "    # Compare each record with subsequent records for similarity\n",
    "    for i in range(n):\n",
    "        if group_ids[i] != -1:\n",
    "            continue  # Skip if already assigned to a group\n",
    "        group_ids[i] = current_group  # Assign current record to a new group\n",
    "        base_text = df.iloc[i]['cleaned_discovery']\n",
    "        for j in range(i + 1, n):\n",
    "            if group_ids[j] != -1:\n",
    "                continue\n",
    "            compare_text = df.iloc[j]['cleaned_discovery']\n",
    "            similarity = fuzz.ratio(base_text, compare_text)\n",
    "            # If similarity exceeds the threshold, consider them duplicates\n",
    "            if similarity >= threshold:\n",
    "                group_ids[j] = current_group\n",
    "        current_group += 1\n",
    "    \n",
    "    # Add the computed group ID to the DataFrame\n",
    "    df['group_id'] = group_ids\n",
    "\n",
    "    # Function to aggregate group records\n",
    "    def aggregate_group(sub_df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Aggregate records within a duplicate group.\n",
    "        - For the discovery name, use the record with the shortest cleaned text (as a representative).\n",
    "        - For other fields, take the union of values.\n",
    "        \"\"\"\n",
    "        # Choose representative record (with the shortest cleaned discovery string)\n",
    "        rep = sub_df.loc[sub_df['cleaned_discovery'].str.len().idxmin()]\n",
    "        # Aggregate Year of Invention by taking the union and joining with semicolon\n",
    "        years = ';'.join(sorted(set(sub_df['Year of Invention'].astype(str))))\n",
    "        # Aggregate Name of Inventor\n",
    "        inventors = ';'.join(sorted(set(sub_df['Name of Inventor'])))\n",
    "        # Aggregate Nationality\n",
    "        nationalities = ';'.join(sorted(set(sub_df['Nationality'])))\n",
    "        return pd.Series({\n",
    "            'Year of Invention': years,\n",
    "            'Name of Inventor': inventors,\n",
    "            'Name of Scientific Discovery': rep['Name of Scientific Discovery'],\n",
    "            'Nationality': nationalities\n",
    "        })\n",
    "    \n",
    "    # Group by the duplicate group and aggregate\n",
    "    deduped_df = df.groupby('group_id').apply(aggregate_group).reset_index(drop=True)\n",
    "    return deduped_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete. Results saved to 'deduped_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Main function to execute the data cleaning and deduplication\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Read the CSV files into pandas DataFrames\n",
    "    df1 = pd.read_csv('raw_data/clean_data/cleaned_data_1.csv')\n",
    "    df2 = pd.read_csv('raw_data/clean_data/cleaned_data_2.csv')\n",
    "    \n",
    "    # Concatenate the two datasets\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    # Apply deduplication on the combined data with a chosen similarity threshold\n",
    "    deduped_df = deduplicate_discoveries(combined_df, threshold=90)\n",
    "    \n",
    "    # Save the deduplicated data to a new CSV file\n",
    "    deduped_df.to_csv('raw_data/clean_data/deduped_data.csv', index=False)\n",
    "    print(\"Deduplication complete. Results saved to 'deduped_data.csv'.\")\n",
    "\n",
    "# Run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
